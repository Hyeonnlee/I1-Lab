"""
Hugging Face ë¬´ë£Œ ì„ë² ë”© ëª¨ë¸ ê¸°ë°˜ RAG SQL Generator
ChromaDB + Hugging Face Embedding Models + Inference API
"""

import requests
import json
from typing import Dict, List, Any, Optional
import chromadb
from chromadb.config import Settings
from chromadb.utils import embedding_functions
import os
import hashlib
from dataclasses import dataclass
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


@dataclass
class TableSchema:
    """í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ì˜"""
    name: str
    columns: List[str]
    description: str
    sample_queries: List[str] = None


@dataclass
class KPIDefinition:
    """KPI ì •ì˜"""
    name: str
    formula: str
    description: str
    unit: str
    table: str
    related_terms: List[str] = None


@dataclass
class GoldenSQL:
    """ê²€ì¦ëœ SQL ì˜ˆì œ"""
    question: str
    sql: str
    explanation: str
    tags: List[str]


class HuggingFaceEmbedding:
    """Hugging Face ì„ë² ë”© ëª¨ë¸ ë˜í¼"""
    
    # ì¶”ì²œ ë¬´ë£Œ ì„ë² ë”© ëª¨ë¸ ëª©ë¡
    EMBEDDING_MODELS = {
        'bge_small_en': {
            'name': 'BAAI/bge-small-en-v1.5',
            'dimension': 384,
            'description': 'ê°€ë³ê³  ë¹ ë¥¸ ì˜ì–´ ì„ë² ë”© (ì¶”ì²œ)',
            'language': 'English'
        },
        'bge_base_en': {
            'name': 'BAAI/bge-base-en-v1.5',
            'dimension': 768,
            'description': 'ê· í˜•ì¡íŒ ì˜ì–´ ì„ë² ë”©',
            'language': 'English'
        },
        'bge_large_en': {
            'name': 'BAAI/bge-large-en-v1.5',
            'dimension': 1024,
            'description': 'ê³ ì„±ëŠ¥ ì˜ì–´ ì„ë² ë”©',
            'language': 'English'
        },
        'bge_m3': {
            'name': 'BAAI/bge-m3',
            'dimension': 1024,
            'description': 'ë‹¤êµ­ì–´ ì„ë² ë”© (í•œêµ­ì–´ í¬í•¨)',
            'language': 'Multilingual'
        },
        'multilingual_e5_small': {
            'name': 'intfloat/multilingual-e5-small',
            'dimension': 384,
            'description': 'ë‹¤êµ­ì–´ ì†Œí˜• ëª¨ë¸ (í•œêµ­ì–´ ì§€ì›)',
            'language': 'Multilingual'
        },
        'multilingual_e5_base': {
            'name': 'intfloat/multilingual-e5-base',
            'dimension': 768,
            'description': 'ë‹¤êµ­ì–´ ê¸°ë³¸ ëª¨ë¸ (í•œêµ­ì–´ ìš°ìˆ˜)',
            'language': 'Multilingual'
        },
        'multilingual_e5_large': {
            'name': 'intfloat/multilingual-e5-large',
            'dimension': 1024,
            'description': 'ë‹¤êµ­ì–´ ëŒ€í˜• ëª¨ë¸ (ìµœê³  ì„±ëŠ¥)',
            'language': 'Multilingual'
        },
        'gte_small': {
            'name': 'thenlper/gte-small',
            'dimension': 384,
            'description': 'ê²½ëŸ‰ ë²”ìš© ì„ë² ë”©',
            'language': 'English'
        },
        'gte_base': {
            'name': 'thenlper/gte-base',
            'dimension': 768,
            'description': 'ë²”ìš© ì„ë² ë”©',
            'language': 'English'
        },
        'all_minilm': {
            'name': 'sentence-transformers/all-MiniLM-L6-v2',
            'dimension': 384,
            'description': 'ì´ˆê²½ëŸ‰ ë¹ ë¥¸ ì„ë² ë”©',
            'language': 'English'
        },
        'paraphrase_multilingual': {
            'name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
            'dimension': 384,
            'description': 'ë‹¤êµ­ì–´ ê²½ëŸ‰ ì„ë² ë”©',
            'language': 'Multilingual'
        }
    }
    
    def __init__(self, model_key: str = 'multilingual_e5_base', hf_token: str = None):
        """
        ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        
        Args:
            model_key: EMBEDDING_MODELSì˜ í‚¤
            hf_token: Hugging Face API í† í°
        """
        self.model_key = model_key
        self.model_info = self.EMBEDDING_MODELS.get(model_key)
        
        if not self.model_info:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_key}")
        
        self.model_name = self.model_info['name']
        self.dimension = self.model_info['dimension']
        self.hf_token = hf_token or os.getenv("HUGGINGFACE_API_KEY")
        
        print(f"\nğŸ¯ ì„ë² ë”© ëª¨ë¸: {self.model_name}")
        print(f"   - ì„¤ëª…: {self.model_info['description']}")
        print(f"   - ì–¸ì–´: {self.model_info['language']}")
        print(f"   - ì°¨ì›: {self.dimension}D")
    
    @classmethod
    def list_models(cls):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥"""
        print("\n" + "="*70)
        print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ëª¨ë¸")
        print("="*70)
        
        for key, info in cls.EMBEDDING_MODELS.items():
            lang_emoji = "ğŸŒ" if info['language'] == 'Multilingual' else "ğŸ‡¬ğŸ‡§"
            print(f"\n{lang_emoji} [{key}]")
            print(f"   ëª¨ë¸: {info['name']}")
            print(f"   ì„¤ëª…: {info['description']}")
            print(f"   ì°¨ì›: {info['dimension']}D")
        
        print("\n" + "="*70)
        print("ğŸ’¡ ì¶”ì²œ:")
        print("   - í•œêµ­ì–´ ì‚¬ìš©: multilingual_e5_base (ê· í˜•)")
        print("   - ë¹ ë¥¸ ì†ë„: multilingual_e5_small (ê°€ë²¼ì›€)")
        print("   - ìµœê³  í’ˆì§ˆ: multilingual_e5_large (ëŠë¦¼)")
        print("   - ì˜ì–´ë§Œ: bge_small_en (ìµœê³  ì†ë„)")
        print("="*70 + "\n")
    
    def create_embedding_function(self):
        """ChromaDBìš© ì„ë² ë”© í•¨ìˆ˜ ìƒì„±"""
        # ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© (sentence-transformers) - ë” ì•ˆì •ì 
        try:
            from sentence_transformers import SentenceTransformer
            
            class LocalEmbeddingFunction:
                def __init__(self, model_name):
                    print(f"  ğŸ“¥ ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì²˜ìŒë§Œ ì‹œê°„ ì†Œìš”)")
                    try:
                        self.model = SentenceTransformer(model_name)
                        print(f"  âœ“ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name}")
                    except Exception as e:
                        print(f"  âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                        raise
                
                def __call__(self, input):
                    if isinstance(input, str):
                        input = [input]
                    try:
                        embeddings = self.model.encode(input, convert_to_numpy=True)
                        return embeddings.tolist()
                    except Exception as e:
                        print(f"  âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
                        raise
            
            return LocalEmbeddingFunction(self.model_name)
        
        except ImportError:
            print("\n" + "="*70)
            print("âŒ sentence-transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            print("="*70)
            print("\nì„¤ì¹˜ ë°©ë²•:")
            print("  pip install sentence-transformers torch")
            print("\në˜ëŠ” conda ì‚¬ìš©:")
            print("  conda install -c conda-forge sentence-transformers")
            print("\n" + "="*70)
            raise ImportError("sentence-transformers íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤")


class VectorDBManager:
    """ë²¡í„° DB ê´€ë¦¬ì (ì„ë² ë”© ëª¨ë¸ ì„ íƒ ê°€ëŠ¥)"""
    
    def __init__(self, 
                 embedding_model: str = 'multilingual_e5_base',
                 hf_token: str = None,
                 persist_directory: str = "./chroma_db_hf"):
        """
        ì´ˆê¸°í™”
        
        Args:
            embedding_model: ì„ë² ë”© ëª¨ë¸ í‚¤
            hf_token: Hugging Face í† í°
            persist_directory: ì €ì¥ ê²½ë¡œ
        """
        self.embedding_model_key = embedding_model
        self.persist_directory = persist_directory
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embedding = HuggingFaceEmbedding(embedding_model, hf_token)
        self.embedding_function = self.embedding.create_embedding_function()
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # ì»¬ë ‰ì…˜ ìƒì„± (ì„ë² ë”© í•¨ìˆ˜ ì ìš©)
        self.metadata_collection = self._get_or_create_collection(
            "metadata_rag", self.embedding_function
        )
        self.business_collection = self._get_or_create_collection(
            "business_logic_rag", self.embedding_function
        )
        self.fewshot_collection = self._get_or_create_collection(
            "fewshot_sql_rag", self.embedding_function
        )
        
        print(f"âœ“ ChromaDB ì´ˆê¸°í™” ì™„ë£Œ (ì €ì¥ ê²½ë¡œ: {persist_directory})")
    
    def _get_or_create_collection(self, name: str, embedding_function):
        """ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±"""
        try:
            return self.client.get_collection(
                name=name,
                embedding_function=embedding_function
            )
        except:
            return self.client.create_collection(
                name=name,
                embedding_function=embedding_function,
                metadata={"hnsw:space": "cosine"}
            )
    
    def reset_all(self):
        """ëª¨ë“  ì»¬ë ‰ì…˜ ì´ˆê¸°í™”"""
        try:
            self.client.delete_collection("metadata_rag")
            self.client.delete_collection("business_logic_rag")
            self.client.delete_collection("fewshot_sql_rag")
        except:
            pass
        
        self.metadata_collection = self._get_or_create_collection(
            "metadata_rag", self.embedding_function
        )
        self.business_collection = self._get_or_create_collection(
            "business_logic_rag", self.embedding_function
        )
        self.fewshot_collection = self._get_or_create_collection(
            "fewshot_sql_rag", self.embedding_function
        )
        
        print("âœ“ ëª¨ë“  ì»¬ë ‰ì…˜ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    def add_metadata(self, tables: List[TableSchema]):
        """ë©”íƒ€ë°ì´í„° ì¶”ê°€"""
        documents = []
        metadatas = []
        ids = []
        
        for table in tables:
            doc_text = f"""
í…Œì´ë¸”ëª…: {table.name}
ì„¤ëª…: {table.description}
ì»¬ëŸ¼: {', '.join(table.columns)}
ìƒ˜í”Œ ì¿¼ë¦¬: {', '.join(table.sample_queries or [])}
            """.strip()
            
            documents.append(doc_text)
            metadatas.append({
                'type': 'table_schema',
                'table_name': table.name,
                'description': table.description
            })
            ids.append(f"table_{table.name}")
        
        if documents:
            try:
                self.metadata_collection.add(
                    documents=documents,
                    metadatas=metadatas,
                    ids=ids
                )
                print(f"âœ“ ë©”íƒ€ë°ì´í„° {len(documents)}ê°œ ì¶”ê°€ ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ ë©”íƒ€ë°ì´í„° ì¶”ê°€ ì‹¤íŒ¨: {e}")
                raise
    
    def add_business_logic(self, kpis: Dict[str, KPIDefinition]):
        """ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì¶”ê°€"""
        documents = []
        metadatas = []
        ids = []
        
        for key, kpi in kpis.items():
            related_terms_str = ', '.join(kpi.related_terms or [])
            doc_text = f"""
KPIëª…: {kpi.name}
ë©”íŠ¸ë¦­: {key}
ì„¤ëª…: {kpi.description}
ê³µì‹: {kpi.formula}
ë‹¨ìœ„: {kpi.unit}
ê´€ë ¨ ìš©ì–´: {related_terms_str}
í…Œì´ë¸”: {kpi.table}
            """.strip()
            
            documents.append(doc_text)
            metadatas.append({
                'type': 'kpi_definition',
                'metric': key,
                'name': kpi.name,
                'formula': kpi.formula
            })
            ids.append(f"kpi_{key}")
        
        if documents:
            self.business_collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
            print(f"âœ“ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ {len(documents)}ê°œ ì¶”ê°€ ì™„ë£Œ")
    
    def add_golden_sqls(self, sqls: List[GoldenSQL]):
        """Golden SQL ì¶”ê°€"""
        documents = []
        metadatas = []
        ids = []
        
        for sql_obj in sqls:
            doc_text = f"""
ì§ˆë¬¸: {sql_obj.question}
ì„¤ëª…: {sql_obj.explanation}
íƒœê·¸: {', '.join(sql_obj.tags)}
            """.strip()
            
            documents.append(doc_text)
            metadatas.append({
                'type': 'golden_sql',
                'question': sql_obj.question,
                'tags': ','.join(sql_obj.tags),
                'sql': sql_obj.sql
            })
            
            sql_id = hashlib.md5(sql_obj.question.encode()).hexdigest()
            ids.append(f"sql_{sql_id}")
        
        if documents:
            self.fewshot_collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
            print(f"âœ“ Golden SQL {len(documents)}ê°œ ì¶”ê°€ ì™„ë£Œ")
    
    def search_metadata(self, query: str, top_k: int = 3) -> List[Dict]:
        """ë©”íƒ€ë°ì´í„° ê²€ìƒ‰"""
        results = self.metadata_collection.query(
            query_texts=[query],
            n_results=top_k
        )
        return self._format_results(results)
    
    def search_business_logic(self, query: str, top_k: int = 3) -> List[Dict]:
        """ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ê²€ìƒ‰"""
        results = self.business_collection.query(
            query_texts=[query],
            n_results=top_k
        )
        return self._format_results(results)
    
    def search_golden_sqls(self, query: str, top_k: int = 3) -> List[Dict]:
        """Golden SQL ê²€ìƒ‰"""
        results = self.fewshot_collection.query(
            query_texts=[query],
            n_results=top_k
        )
        return self._format_results(results)
    
    def _format_results(self, results: Dict) -> List[Dict]:
        """ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ…"""
        formatted = []
        
        if not results['ids'] or not results['ids'][0]:
            return formatted
        
        for i in range(len(results['ids'][0])):
            formatted.append({
                'id': results['ids'][0][i],
                'document': results['documents'][0][i],
                'metadata': results['metadatas'][0][i],
                'distance': results['distances'][0][i] if 'distances' in results else None
            })
        
        return formatted
    
    def get_collection_stats(self) -> Dict[str, int]:
        """ì»¬ë ‰ì…˜ í†µê³„"""
        return {
            'metadata': self.metadata_collection.count(),
            'business_logic': self.business_collection.count(),
            'golden_sql': self.fewshot_collection.count()
        }


class HuggingFaceSQLAgent:
    """Hugging Face ê¸°ë°˜ SQL Generator (ì„ë² ë”© ëª¨ë¸ ì„ íƒ ê°€ëŠ¥)"""
    
    # LLM ëª¨ë¸ ëª©ë¡
    LLM_MODELS = {
        'qwen_coder_32b': 'Qwen/Qwen2.5-Coder-32B-Instruct',
        'deepseek_33b': 'deepseek-ai/deepseek-coder-33b-instruct',
        'codellama_34b': 'codellama/CodeLlama-34b-Instruct-hf',
        'mistral_7b': 'mistralai/Mistral-7B-Instruct-v0.3',
        'qwen_7b': 'Qwen/Qwen2.5-7B-Instruct',
        'phi3_medium': 'microsoft/Phi-3-medium-128k-instruct',
    }
    
    def __init__(self, 
                 hf_token: str = None,
                 embedding_model: str = 'multilingual_e5_base',
                 llm_model: str = 'mistral_7b'):
        """
        ì´ˆê¸°í™”
        
        Args:
            hf_token: Hugging Face API í† í°
            embedding_model: ì„ë² ë”© ëª¨ë¸ í‚¤
            llm_model: LLM ëª¨ë¸ í‚¤
        """
        self.hf_token = hf_token or os.environ.get("HF_TOKEN")
        
        # LLM ëª¨ë¸ ì„¤ì •
        if llm_model in self.LLM_MODELS:
            self.llm_model_id = self.LLM_MODELS[llm_model]
        else:
            self.llm_model_id = llm_model
        
        print(f"\nğŸ¤– LLM ëª¨ë¸: {self.llm_model_id}")
        
        # API ì„¤ì •
        self.api_url = f"https://api-inference.huggingface.co/models/{self.llm_model_id}"
        self.headers = {"Authorization": f"Bearer {self.hf_token}"} if self.hf_token else {}
        
        # ë²¡í„° DB ì´ˆê¸°í™” (ì„ë² ë”© ëª¨ë¸ ì ìš©)
        print("\n" + "="*70)
        print("ğŸ”§ ë²¡í„° DB ì´ˆê¸°í™” ì¤‘...")
        print("="*70)
        
        self.vector_db = VectorDBManager(
            embedding_model=embedding_model,
            hf_token=self.hf_token
        )
        
        # ë°ì´í„° ì´ˆê¸°í™”
        self._initialize_data()
        
        print("\n" + "="*70)
        print("âœ… SQL Generator Agent ì´ˆê¸°í™” ì™„ë£Œ!")
        print("="*70)
        
        stats = self.vector_db.get_collection_stats()
        print(f"\nğŸ“Š ë²¡í„° DB í†µê³„:")
        print(f"   - ë©”íƒ€ë°ì´í„°: {stats['metadata']}ê°œ")
        print(f"   - ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§: {stats['business_logic']}ê°œ")
        print(f"   - Golden SQL: {stats['golden_sql']}ê°œ\n")
    
    def _initialize_data(self):
        """RAG ë°ì´í„° ì´ˆê¸°í™”"""
        # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
        stats = self.vector_db.get_collection_stats()
        if stats['metadata'] > 0:
            print("âœ“ ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©")
            return
        
        print("\nğŸ“¥ RAG ë°ì´í„° ì´ˆê¸°í™” ì¤‘...")
        
        # 1. í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ
        tables = [
            TableSchema(
                name='defect_records',
                columns=['record_id', 'line_id', 'defect_count', 'total_count', 
                        'defect_rate', 'record_date', 'shift', 'operator_id'],
                description='ë¶ˆëŸ‰ ê¸°ë¡ í…Œì´ë¸”. ì¼ë³„/ì‹œí”„íŠ¸ë³„ ë¶ˆëŸ‰ë¥  ë°ì´í„° ì €ì¥',
                sample_queries=['ë¶ˆëŸ‰ë¥  ì¶”ì„¸', 'ë¶ˆëŸ‰ ê±´ìˆ˜', 'ë¼ì¸ë³„ ë¶ˆëŸ‰']
            ),
            TableSchema(
                name='process_variables',
                columns=['var_id', 'line_id', 'temperature', 'pressure', 
                        'speed', 'humidity', 'timestamp'],
                description='ê³µì • ë³€ìˆ˜ í…Œì´ë¸”. ì˜¨ë„, ì••ë ¥, ì†ë„ ë“± ì‹¤ì‹œê°„ ê³µì • ë°ì´í„°',
                sample_queries=['ì˜¨ë„ ë°ì´í„°', 'ì••ë ¥ ì¶”ì´', 'ê³µì • ë³€ìˆ˜']
            ),
            TableSchema(
                name='production_lines',
                columns=['line_id', 'line_name', 'factory_id', 'status'],
                description='ìƒì‚° ë¼ì¸ ë§ˆìŠ¤í„° í…Œì´ë¸”',
                sample_queries=['ë¼ì¸ ì •ë³´', 'ê°€ë™ ë¼ì¸']
            ),
            TableSchema(
                name='quality_events',
                columns=['event_id', 'line_id', 'event_type', 'severity', 
                        'description', 'occurred_at'],
                description='í’ˆì§ˆ ì´ë²¤íŠ¸ ë¡œê·¸',
                sample_queries=['í’ˆì§ˆ ì´ìŠˆ', 'ì´ìƒ ë°œìƒ']
            )
        ]
        self.vector_db.add_metadata(tables)
        
        # 2. KPI ì •ì˜
        kpis = {
            'DEFECT_RATE': KPIDefinition(
                name='ë¶ˆëŸ‰ë¥ ',
                formula='(defect_count / total_count) * 100',
                description='ë¶ˆëŸ‰ë¥  = (ë¶ˆëŸ‰ ìˆ˜ / ì „ì²´ ìƒì‚° ìˆ˜) Ã— 100',
                unit='%',
                table='defect_records',
                related_terms=['ë¶ˆëŸ‰', 'í’ˆì§ˆ', 'ê²°í•¨', 'defect', 'quality']
            ),
            'CORRELATION': KPIDefinition(
                name='ìƒê´€ê´€ê³„',
                formula='CORR(variable1, variable2)',
                description='ë‘ ë³€ìˆ˜ ê°„ì˜ ìƒê´€ê³„ìˆ˜ (-1 ~ 1)',
                unit='coefficient',
                table='process_variables',
                related_terms=['ìƒê´€', 'correlation', 'ê´€ê³„', 'ì˜í–¥']
            )
        }
        self.vector_db.add_business_logic(kpis)
        
        # 3. Golden SQL
        golden_sqls = [
            GoldenSQL(
                question="ì§€ë‚œ 3ê°œì›”ê°„ ë¶ˆëŸ‰ë¥  ì¶”ì„¸ ë¶„ì„",
                sql="""SELECT 
    DATE_FORMAT(record_date, '%Y-%m') AS month,
    AVG(defect_rate) AS avg_defect_rate,
    MIN(defect_rate) AS min_defect_rate,
    MAX(defect_rate) AS max_defect_rate,
    STDDEV(defect_rate) AS stddev_defect_rate
FROM defect_records
WHERE line_id = 'A'
    AND record_date >= DATE_SUB(CURRENT_DATE, INTERVAL 3 MONTH)
GROUP BY DATE_FORMAT(record_date, '%Y-%m')
ORDER BY month;""",
                explanation="ì›”ë³„ ë¶ˆëŸ‰ë¥  í†µê³„ ì§‘ê³„",
                tags=['trend', 'aggregation', 'defect_rate', 'monthly']
            ),
            GoldenSQL(
                question="ë¶ˆëŸ‰ë¥ ê³¼ ê³µì • ë³€ìˆ˜ ìƒê´€ê´€ê³„ ë¶„ì„",
                sql="""WITH defect_stats AS (
    SELECT 
        DATE(record_date) AS date,
        AVG(defect_rate) AS avg_defect_rate
    FROM defect_records
    WHERE line_id = 'A'
        AND record_date >= DATE_SUB(CURRENT_DATE, INTERVAL 3 MONTH)
    GROUP BY DATE(record_date)
),
process_stats AS (
    SELECT
        DATE(timestamp) AS date,
        AVG(temperature) AS avg_temp,
        AVG(pressure) AS avg_pressure
    FROM process_variables
    WHERE line_id = 'A'
        AND timestamp >= DATE_SUB(CURRENT_DATE, INTERVAL 3 MONTH)
    GROUP BY DATE(timestamp)
)
SELECT
    'temperature' AS variable,
    CORR(d.avg_defect_rate, p.avg_temp) AS correlation
FROM defect_stats d
JOIN process_stats p ON d.date = p.date
UNION ALL
SELECT
    'pressure' AS variable,
    CORR(d.avg_defect_rate, p.avg_pressure) AS correlation
FROM defect_stats d
JOIN process_stats p ON d.date = p.date;""",
                explanation="CTEì™€ CORR í•¨ìˆ˜ë¡œ ìƒê´€ê´€ê³„ ë¶„ì„",
                tags=['correlation', 'cte', 'join', 'causality']
            ),
            GoldenSQL(
                question="ì´ë™í‰ê· ìœ¼ë¡œ ë¶ˆëŸ‰ë¥  ì´ìƒì¹˜ íƒì§€",
                sql="""WITH daily_metrics AS (
    SELECT
        record_date,
        defect_rate,
        AVG(defect_rate) OVER (
            ORDER BY record_date 
            ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
        ) AS moving_avg_7days,
        STDDEV(defect_rate) OVER (
            ORDER BY record_date 
            ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
        ) AS moving_stddev
    FROM defect_records
    WHERE line_id = 'A'
        AND record_date >= DATE_SUB(CURRENT_DATE, INTERVAL 3 MONTH)
)
SELECT
    record_date,
    defect_rate,
    moving_avg_7days,
    CASE
        WHEN defect_rate > moving_avg_7days + (2 * moving_stddev) THEN 'HIGH_ANOMALY'
        WHEN defect_rate < moving_avg_7days - (2 * moving_stddev) THEN 'LOW_ANOMALY'
        ELSE 'NORMAL'
    END AS anomaly_status
FROM daily_metrics
ORDER BY record_date;""",
                explanation="ìœˆë„ìš° í•¨ìˆ˜ë¡œ ì´ë™í‰ê·  ê³„ì‚° í›„ 2-sigma ê¸°ì¤€ ì´ìƒì¹˜ íƒì§€",
                tags=['window_function', 'moving_average', 'anomaly_detection']
            )
        ]
        self.vector_db.add_golden_sqls(golden_sqls)
    
    def generate_sql(self, user_query: str, line_id: str = 'A') -> Dict[str, Any]:
        """ìì—°ì–´ â†’ SQL ìƒì„±"""
        
        print(f"\n{'='*70}")
        print(f"ğŸ“ ì§ˆë¬¸: {user_query}")
        print(f"ğŸ­ ë¼ì¸: {line_id}")
        print(f"{'='*70}\n")
        
        # 1. RAG ê²€ìƒ‰
        print("ğŸ” ìœ ì‚¬ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘...")
        
        print("  [1/3] í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ê²€ìƒ‰...")
        tables = self.vector_db.search_metadata(user_query, top_k=2)
        
        print("  [2/3] KPI ì •ì˜ ê²€ìƒ‰...")
        kpis = self.vector_db.search_business_logic(user_query, top_k=2)
        
        print("  [3/3] Golden SQL ê²€ìƒ‰...")
        examples = self.vector_db.search_golden_sqls(user_query, top_k=2)
        
        print(f"  âœ“ ì´ {len(tables) + len(kpis) + len(examples)}ê°œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì™„ë£Œ\n")
        
        # 2. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self._build_prompt(user_query, line_id, tables, kpis, examples)
        
        # 3. LLM í˜¸ì¶œ
        print("ğŸ¤– SQL ìƒì„± ì¤‘...")
        response = self._generate_sql_with_llm(prompt)
        
        # 4. SQL ì¶”ì¶œ
        sql = self._extract_sql(response)
        
        print("âœ… SQL ìƒì„± ì™„ë£Œ!\n")
        
        return {
            'user_query': user_query,
            'line_id': line_id,
            'sql': sql,
            'raw_response': response,
            'rag_context': {
                'tables': tables,
                'kpis': kpis,
                'examples': examples
            }
        }
    
    def _build_prompt(self, query: str, line_id: str, 
                     tables: List[Dict], kpis: List[Dict], examples: List[Dict]) -> str:
        """í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        # ìŠ¤í‚¤ë§ˆ ì •ë³´
        schema_text = "## ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ\n\n"
        for t in tables:
            schema_text += f"### {t['metadata'].get('table_name', 'Unknown')}\n"
            schema_text += f"{t['document']}\n\n"
        
        # KPI ì •ë³´
        kpi_text = "## KPI ì •ì˜\n\n"
        for k in kpis:
            kpi_text += f"### {k['metadata'].get('name', 'KPI')}\n"
            kpi_text += f"{k['document']}\n\n"
        
        # ì˜ˆì œ SQL
        example_text = "## ì°¸ê³  SQL ì˜ˆì œ\n\n"
        for i, ex in enumerate(examples, 1):
            example_text += f"### ì˜ˆì œ {i}\n"
            example_text += f"```sql\n{ex['metadata'].get('sql', '')}\n```\n\n"
        
        prompt = f"""ë‹¹ì‹ ì€ SQL ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìì—°ì–´ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì •í™•í•œ SQL ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.

{schema_text}

{kpi_text}

{example_text}

## ìš”ì²­
ì§ˆë¬¸: {query}
ëŒ€ìƒ ë¼ì¸: {line_id}

ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ line_id = '{line_id}' ì¡°ê±´ì„ í¬í•¨í•œ SQL ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
ë°˜ë“œì‹œ ```sql ì½”ë“œ ë¸”ë¡ ì•ˆì— SQLë§Œ ì‘ì„±í•˜ì„¸ìš”.

```sql
"""
        
        return prompt
    
    def _generate_sql_with_llm(self, prompt: str) -> str:
        """LLMìœ¼ë¡œ SQL ìƒì„±"""
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": 1000,
                "temperature": 0.1,
                "top_p": 0.95,
                "do_sample": True,
                "return_full_text": False
            }
        }
        
        try:
            response = requests.post(
                self.api_url,
                headers=self.headers,
                json=payload,
                timeout=60
            )
            response.raise_for_status()
            
            result = response.json()
            
            if isinstance(result, list) and len(result) > 0:
                return result[0].get('generated_text', '')
            elif isinstance(result, dict):
                return result.get('generated_text', '')
            else:
                return str(result)
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ API ì˜¤ë¥˜: {e}")
            return "-- SQL ìƒì„± ì‹¤íŒ¨"
    
    def _extract_sql(self, response: str) -> str:
        """ì‘ë‹µì—ì„œ SQL ì¶”ì¶œ"""
        # ```sql ... ``` ë¸”ë¡ ì°¾ê¸°
        if "```sql" in response:
            start = response.find("```sql") + 6
            end = response.find("```", start)
            if end != -1:
                return response[start:end].strip()
        
        # ``` ... ``` ë¸”ë¡ ì°¾ê¸°
        if "```" in response:
            start = response.find("```") + 3
            end = response.find("```", start)
            if end != -1:
                return response[start:end].strip()
        
        # ì „ì²´ ì‘ë‹µ ë°˜í™˜
        return response.strip()
    
    def print_result(self, result: Dict[str, Any]):
        """ê²°ê³¼ ì¶œë ¥"""
        print("="*70)
        print("ğŸ“Š SQL ìƒì„± ê²°ê³¼")
        print("="*70)
        print(f"\nğŸ“ ì§ˆë¬¸: {result['user_query']}")
        print(f"ğŸ­ ë¼ì¸: {result['line_id']}")
        
        print("\n" + "-"*70)
        print("ğŸ’¾ ìƒì„±ëœ SQL:")
        print("-"*70)
        print(result['sql'])
        
        print("\n" + "-"*70)
        print("ğŸ” ì‚¬ìš©ëœ RAG ì»¨í…ìŠ¤íŠ¸:")
        print("-"*70)
        ctx = result['rag_context']
        print(f"  - í…Œì´ë¸”: {len(ctx['tables'])}ê°œ")
        print(f"  - KPI: {len(ctx['kpis'])}ê°œ")
        print(f"  - ì˜ˆì œ SQL: {len(ctx['examples'])}ê°œ")
        
        print("\n" + "="*70 + "\n")
    
    def interactive_mode(self):
        """ëŒ€í™”í˜• ëª¨ë“œ"""
        print("\n" + "="*70)
        print("ğŸš€ ëŒ€í™”í˜• SQL Generator")
        print("="*70)
        print("\nëª…ë ¹ì–´:")
        print("  - ì§ˆë¬¸ ì…ë ¥: SQL ìƒì„±")
        print("  - 'quit' / 'exit': ì¢…ë£Œ")
        print("  - 'reset': ë²¡í„° DB ì´ˆê¸°í™”")
        print("  - 'stats': í†µê³„ ë³´ê¸°")
        print("  - 'models': ì„ë² ë”© ëª¨ë¸ ëª©ë¡")
        print()
        
        while True:
            try:
                query = input("\nğŸ’¬ ì§ˆë¬¸: ").strip()
                
                if not query:
                    continue
                
                if query.lower() in ['quit', 'exit']:
                    print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.\n")
                    break
                
                if query.lower() == 'reset':
                    self.vector_db.reset_all()
                    self._initialize_data()
                    print("âœ“ ë²¡í„° DB ì´ˆê¸°í™” ë° ë°ì´í„° ì¬ë¡œë”© ì™„ë£Œ")
                    continue
                
                if query.lower() == 'stats':
                    stats = self.vector_db.get_collection_stats()
                    print(f"\nğŸ“Š ë²¡í„° DB í†µê³„:")
                    print(f"  - ë©”íƒ€ë°ì´í„°: {stats['metadata']}ê°œ")
                    print(f"  - ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§: {stats['business_logic']}ê°œ")
                    print(f"  - Golden SQL: {stats['golden_sql']}ê°œ")
                    continue
                
                if query.lower() == 'models':
                    HuggingFaceEmbedding.list_models()
                    continue
                
                line_id = input("ğŸ­ ë¼ì¸ ID (ê¸°ë³¸: A): ").strip() or 'A'
                
                result = self.generate_sql(query, line_id)
                self.print_result(result)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.\n")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜: {str(e)}\n")


# =============================================================================
# ë©”ì¸ ì‹¤í–‰
# =============================================================================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ¤— Hugging Face ë¬´ë£Œ ëª¨ë¸ ê¸°ë°˜ RAG SQL Generator               â•‘
â•‘                                                                  â•‘
â•‘  - ì„ë² ë”© ëª¨ë¸: 11ê°€ì§€ ë¬´ë£Œ ëª¨ë¸ ì„ íƒ ê°€ëŠ¥                      â•‘
â•‘  - LLM ëª¨ë¸: 6ê°€ì§€ ì½”ë“œ ìƒì„± ëª¨ë¸ ì„ íƒ ê°€ëŠ¥                     â•‘
â•‘  - RAG: ChromaDB + ì‹œë§¨í‹± ê²€ìƒ‰                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    # HF í† í°
    hf_token = os.environ.get("HF_TOKEN")
    
    if not hf_token:
        print("âš ï¸  HF_TOKEN í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   https://huggingface.co/settings/tokens ì—ì„œ ë¬´ë£Œ í† í° ë°œê¸‰")
        print("   export HF_TOKEN='hf_...'")
        print()
        hf_token = input("HF í† í° ì…ë ¥ (Enter=ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©): ").strip()
        if not hf_token:
            print("\nğŸ’¡ ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤ (sentence-transformers í•„ìš”)")
            print("   pip install sentence-transformers\n")
    
    # ì„ë² ë”© ëª¨ë¸ ì„ íƒ
    print("\n" + "="*70)
    print("ğŸ“‹ ì„ë² ë”© ëª¨ë¸ ì„ íƒ")
    print("="*70)
    print("\nì¶”ì²œ ëª¨ë¸:")
    print("1. multilingual_e5_base - ë‹¤êµ­ì–´ ê· í˜•í˜• (ì¶”ì²œ) â­")
    print("2. multilingual_e5_small - ë‹¤êµ­ì–´ ê²½ëŸ‰ (ë¹ ë¦„)")
    print("3. multilingual_e5_large - ë‹¤êµ­ì–´ ëŒ€í˜• (ê³ ì„±ëŠ¥)")
    print("4. bge_small_en - ì˜ì–´ ê²½ëŸ‰ (ë§¤ìš° ë¹ ë¦„)")
    print("5. bge_m3 - ë‹¤êµ­ì–´ ê³ ì„±ëŠ¥")
    print("6. all - ì „ì²´ ëª¨ë¸ ëª©ë¡ ë³´ê¸°")
    
    emb_choice = input("\nì„ë² ë”© ëª¨ë¸ ì„ íƒ (1-6, ê¸°ë³¸=1): ").strip() or '1'
    
    if emb_choice == '6':
        HuggingFaceEmbedding.list_models()
        emb_model = input("\nëª¨ë¸ í‚¤ ì…ë ¥: ").strip() or 'multilingual_e5_base'
    else:
        emb_map = {
            '1': 'multilingual_e5_base',
            '2': 'multilingual_e5_small',
            '3': 'multilingual_e5_large',
            '4': 'bge_small_en',
            '5': 'bge_m3'
        }
        emb_model = emb_map.get(emb_choice, 'multilingual_e5_base')
    
    # LLM ëª¨ë¸ ì„ íƒ
    print("\n" + "="*70)
    print("ğŸ¤– LLM ëª¨ë¸ ì„ íƒ")
    print("="*70)
    print("\n1. mistral_7b - ê°€ë³ê³  ë¹ ë¦„ (ì¶”ì²œ) â­")
    print("2. qwen_7b - í•œêµ­ì–´ ìš°ìˆ˜")
    print("3. qwen_coder_32b - ìµœê³  ì„±ëŠ¥ (ëŠë¦¼)")
    print("4. deepseek_33b - ì½”ë“œ ìƒì„± íŠ¹í™”")
    
    llm_choice = input("\nLLM ëª¨ë¸ ì„ íƒ (1-4, ê¸°ë³¸=1): ").strip() or '1'
    
    llm_map = {
        '1': 'mistral_7b',
        '2': 'qwen_7b',
        '3': 'qwen_coder_32b',
        '4': 'deepseek_33b'
    }
    llm_model = llm_map.get(llm_choice, 'mistral_7b')
    
    # Agent ì´ˆê¸°í™”
    print("\n" + "="*70)
    print("âš™ï¸  Agent ì´ˆê¸°í™” ì¤‘...")
    print("="*70)
    
    try:
        agent = HuggingFaceSQLAgent(
            hf_token=hf_token if hf_token else None,
            embedding_model=emb_model,
            llm_model=llm_model
        )
    except Exception as e:
        print(f"\nâŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("\nğŸ’¡ í•´ê²° ë°©ë²•:")
        print("   1. HF í† í° í™•ì¸")
        print("   2. sentence-transformers ì„¤ì¹˜: pip install sentence-transformers")
        print("   3. ì¸í„°ë„· ì—°ê²° í™•ì¸")
        return
    
    # ì˜ˆì œ í…ŒìŠ¤íŠ¸
    print("\n" + "="*70)
    print("ğŸ¯ ì˜ˆì œ í…ŒìŠ¤íŠ¸")
    print("="*70)
    
    test_queries = [
        "ì§€ë‚œ 3ê°œì›”ê°„ Aë¼ì¸ì˜ ë¶ˆëŸ‰ë¥  ì¶”ì„¸ë¥¼ ë¶„ì„í•´ì¤˜",
        "ë¶ˆëŸ‰ë¥ ê³¼ ì˜¨ë„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•´ì¤˜"
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n[ì˜ˆì œ {i}/{len(test_queries)}]")
        result = agent.generate_sql(query, line_id='A')
        agent.print_result(result)
        
        if i < len(test_queries):
            input("â¸ï¸  ë‹¤ìŒ ì˜ˆì œ (Enter)...")
    
    # ëŒ€í™”í˜• ëª¨ë“œ
    print("\n" + "="*70)
    user_input = input("ëŒ€í™”í˜• ëª¨ë“œë¡œ ì „í™˜? (y/n): ").strip().lower()
    
    if user_input == 'y':
        agent.interactive_mode()
    else:
        print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.\n")


if __name__ == "__main__":
    main()