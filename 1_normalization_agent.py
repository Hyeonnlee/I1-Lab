import os
import json
import re
from typing import Dict, Any, Optional, List
from huggingface_hub import InferenceClient
from dotenv import load_dotenv
import sys
import io

# í‘œì¤€ ì¶œë ¥(stdout)ì„ UTF-8ë¡œ ì¸ì½”ë”©í•˜ë„ë¡ ì¬ì„¤ì •
if sys.platform == "win32":
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding='utf-8', line_buffering=True)
        sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding='utf-8', line_buffering=True)
    except:
        pass

load_dotenv()

class QueryNormalizationAgent:
    def __init__(self, model_id: str = "mistralai/Mistral-7B-Instruct-v0.2", use_chat: bool = True):
        """
        ê°œì„ ì‚¬í•­:
        1. Chat Completion API ì§€ì› ì¶”ê°€
        2. ì—¬ëŸ¬ ëª¨ë¸ ì˜µì…˜ ì§€ì›
        3. ìë™ fallback ë©”ì»¤ë‹ˆì¦˜
        """
        self.api_token = os.getenv("HUGGINGFACE_API_KEY")
        if not self.api_token:
            raise ValueError("HUGGINGFACE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.model_id = model_id
        self.use_chat = use_chat
        
        # Hugging Face Inference Client ì´ˆê¸°í™”
        try:
            self.client = InferenceClient(token=self.api_token)
            print(f"âœ“ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
            print(f"âœ“ ì‚¬ìš© ëª¨ë¸: {model_id}")
            print(f"âœ“ API ë°©ì‹: {'Chat Completion' if use_chat else 'Text Generation'}")
        except Exception as e:
            print(f"âœ— í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
        
        # ë„ë©”ì¸ ìš©ì–´ì§‘
        self.domain_glossary = {
            "ìˆ˜ìœ¨": "YIELD",
            "3í˜¸ê¸°": "EQUIPMENT_M03",
            "1í˜¸ê¸°": "EQUIPMENT_M01",
            "2í˜¸ê¸°": "EQUIPMENT_M02",
            "ì§€ë‚œì£¼": "LAST_WEEK",
            "ì´ë²ˆì£¼": "THIS_WEEK",
            "ì™œ ë–¨ì–´ì¡Œì–´": "ANOMALY_ANALYSIS",
            "ì•Œë ¤ì¤˜": "DATA_RETRIEVAL",
            "ë³´ì—¬ì¤˜": "DATA_RETRIEVAL",
            "ìƒì‚°ëŸ‰": "PRODUCTION_VOLUME",
            "ë¶ˆëŸ‰ë¥ ": "DEFECT_RATE"
        }

    def _preprocess_query(self, query: str) -> str:
        """1ë‹¨ê³„: ë„ë©”ì¸ ìš©ì–´ì§‘ì„ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ 1ì°¨ ì •ê·œí™”"""
        processed_query = query
        for k, v in self.domain_glossary.items():
            processed_query = processed_query.replace(k, v)
        return processed_query

    def _construct_system_message(self) -> str:
        """ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìƒì„±"""
        return """You are a Manufacturing Query Normalization Agent.
Convert user queries into JSON objects with these exact fields: intent, metric, time_frame, filter.

Rules:
1. 'intent' must be: ANOMALY_ANALYSIS or DATA_RETRIEVAL
2. 'metric' examples: YIELD, PRODUCTION_VOLUME, DEFECT_RATE
3. 'time_frame' has 'type' (RELATIVE or ABSOLUTE) and 'value'
4. 'filter' has 'field' and 'value'
5. Return ONLY valid JSON without any explanation or markdown

Examples:
Input: "ì§€ë‚œì£¼ 3í˜¸ê¸° ìˆ˜ìœ¨ì´ ì™œ ë–¨ì–´ì¡Œì–´?"
Output: {"intent": "ANOMALY_ANALYSIS", "metric": "YIELD", "time_frame": {"type": "RELATIVE", "value": "LAST_WEEK"}, "filter": {"field": "EQUIPMENT_ID", "value": "EQUIPMENT_M03"}}

Input: "ì´ë²ˆì£¼ 1í˜¸ê¸° ìƒì‚°ëŸ‰ ì•Œë ¤ì¤˜"
Output: {"intent": "DATA_RETRIEVAL", "metric": "PRODUCTION_VOLUME", "time_frame": {"type": "RELATIVE", "value": "THIS_WEEK"}, "filter": {"field": "EQUIPMENT_ID", "value": "EQUIPMENT_M01"}}"""

    def _construct_user_message(self, original_query: str) -> str:
        """ì‚¬ìš©ì ë©”ì‹œì§€ ìƒì„±"""
        return f'Convert this query: "{original_query}"'

    def _call_chat_completion(self, original_query: str) -> str:
        """Chat Completion API í˜¸ì¶œ"""
        messages = [
            {"role": "system", "content": self._construct_system_message()},
            {"role": "user", "content": self._construct_user_message(original_query)}
        ]
        
        response = self.client.chat_completion(
            messages=messages,
            model=self.model_id,
            max_tokens=300,
            temperature=0.1
        )
        
        return response.choices[0].message.content

    def _call_text_generation(self, original_query: str) -> str:
        """Text Generation API í˜¸ì¶œ (fallbackìš©)"""
        prompt = f"""{self._construct_system_message()}

{self._construct_user_message(original_query)}

Output:"""
        
        response = self.client.text_generation(
            prompt,
            model=self.model_id,
            max_new_tokens=300,
            temperature=0.1,
            return_full_text=False
        )
        
        return response

    def _clean_json_output(self, text: str) -> str:
        """LLM ì¶œë ¥ì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ê³  ì •ë¦¬"""
        try:
            # Markdown ì½”ë“œ ë¸”ë¡ ì œê±°
            text = re.sub(r'```json\s*', '', text)
            text = re.sub(r'```\s*', '', text)
            
            # JSON íŒ¨í„´ ì°¾ê¸° (ì¤‘ì²©ëœ ê°ì²´ ì§€ì›)
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', text)
            if json_match:
                json_str = json_match.group(0)
                json_str = json_str.replace("'", '"')
                return json_str
            
            # ë°±ì—…: ì²« ë²ˆì§¸ { ë¶€í„° ë§ˆì§€ë§‰ } ê¹Œì§€
            start = text.find('{')
            end = text.rfind('}') + 1
            if start != -1 and end > start:
                json_str = text[start:end]
                json_str = json_str.replace("'", '"')
                return json_str
            
            return text
        except Exception as e:
            print(f"âš  JSON ì •ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
            return text

    def normalize(self, query: str) -> Dict[str, Any]:
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
        print(f"\nğŸ“ ì…ë ¥ ì¿¼ë¦¬: {query}")
        
        # 1. ì „ì²˜ë¦¬
        pre_processed = self._preprocess_query(query)
        print(f"ğŸ”„ ì „ì²˜ë¦¬ ì™„ë£Œ: {pre_processed}")
        
        # 2. LLM í˜¸ì¶œ
        try:
            print(f"ğŸ¤– LLM í˜¸ì¶œ ì¤‘... (ëª¨ë¸: {self.model_id})")
            
            if self.use_chat:
                response = self._call_chat_completion(query)
            else:
                response = self._call_text_generation(query)
            
            print(f"âœ“ LLM ì‘ë‹µ ë°›ìŒ (ê¸¸ì´: {len(response)} chars)")
            print(f"ğŸ“„ ì›ë³¸ ì‘ë‹µ: {response[:200]}...")
            
        except Exception as e:
            error_msg = str(e)
            print(f"âœ— LLM API í˜¸ì¶œ ì‹¤íŒ¨: {error_msg}")
            
            # Chatì—ì„œ ì‹¤íŒ¨í–ˆìœ¼ë©´ Text Generationìœ¼ë¡œ ì¬ì‹œë„
            if self.use_chat and "not supported" in error_msg.lower():
                print("ğŸ”„ Text Generationìœ¼ë¡œ ì¬ì‹œë„...")
                try:
                    response = self._call_text_generation(query)
                    print(f"âœ“ ì¬ì‹œë„ ì„±ê³µ!")
                except Exception as retry_error:
                    return {
                        "error": "All API methods failed",
                        "chat_error": error_msg,
                        "text_gen_error": str(retry_error),
                        "model": self.model_id
                    }
            else:
                return {
                    "error": "LLM API Call Failed",
                    "details": error_msg,
                    "model": self.model_id
                }

        # 3. ê²°ê³¼ íŒŒì‹±
        cleaned_response = self._clean_json_output(response)
        print(f"ğŸ§¹ ì •ë¦¬ëœ ì‘ë‹µ: {cleaned_response}")
        
        try:
            result_json = json.loads(cleaned_response)
            print("âœ“ JSON íŒŒì‹± ì„±ê³µ")
            return result_json
        except json.JSONDecodeError as e:
            print(f"âœ— JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            return {
                "error": "Failed to parse JSON",
                "parsing_error": str(e),
                "raw_output": response[:500],
                "cleaned_output": cleaned_response
            }

# --- ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ---
if __name__ == "__main__":
    print("=" * 50)
    print("ğŸš€ Query Normalization Agent ì‹œì‘")
    print("=" * 50)
    
    # ì—¬ëŸ¬ ëª¨ë¸ ì˜µì…˜ (ìš°ì„ ìˆœìœ„ëŒ€ë¡œ)
    model_options = [
        ("mistralai/Mistral-7B-Instruct-v0.2", True),   # Chat API
        ("meta-llama/Llama-2-7b-chat-hf", True),        # Chat API
        ("google/flan-t5-large", False),                # Text Generation
        ("bigscience/bloom-1b7", False),                # Text Generation (fallback)
    ]
    
    agent = None
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì°¾ê¸°
    for model_id, use_chat in model_options:
        try:
            print(f"\nğŸ” {model_id} ì‹œë„ ì¤‘...")
            agent = QueryNormalizationAgent(model_id=model_id, use_chat=use_chat)
            break
        except Exception as e:
            print(f"âœ— {model_id} ì‹¤íŒ¨: {e}")
            continue
    
    if agent is None:
        print("\nâŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:")
        print("  1. HUGGINGFACE_API_KEYê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€")
        print("  2. ì¸í„°ë„· ì—°ê²° ìƒíƒœ")
        print("  3. Hugging Face ì„œë¹„ìŠ¤ ìƒíƒœ: https://status.huggingface.co/")
        exit(1)
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
    test_queries = [
        "ì§€ë‚œì£¼ 3í˜¸ê¸° ìˆ˜ìœ¨ì´ ì™œ ë–¨ì–´ì¡Œì–´?",
        "ì´ë²ˆì£¼ 1í˜¸ê¸° ìƒì‚°ëŸ‰ ë°ì´í„° ë³´ì—¬ì¤˜"
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n{'='*50}")
        print(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i}")
        print(f"{'='*50}")
        
        result = agent.normalize(query)
        
        print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
        print(json.dumps(result, indent=2, ensure_ascii=False))