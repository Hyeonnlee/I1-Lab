import os
import json
import re
from typing import Dict, Any, Optional, List
from datetime import datetime
from huggingface_hub import InferenceClient
from dotenv import load_dotenv
import sys
import io
from pathlib import Path

# í‘œì¤€ ì¶œë ¥(stdout)ì„ UTF-8ë¡œ ì¸ì½”ë”©í•˜ë„ë¡ ì¬ì„¤ì •
if sys.platform == "win32":
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding='utf-8', line_buffering=True)
        sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding='utf-8', line_buffering=True)
    except:
        pass

load_dotenv()

class DomainGlossaryLoader:
    """ë„ë©”ì¸ ìš©ì–´ì§‘ì„ ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ë¡œë“œí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, glossary_path: Optional[str] = None):
        self.glossary_path = glossary_path or self._find_default_glossary()
        self.glossary: Dict[str, str] = {}
        
    def _find_default_glossary(self) -> str:
        """ê¸°ë³¸ ìš©ì–´ì§‘ íŒŒì¼ ì°¾ê¸°"""
        default_paths = [
            "input/domain_glossary.json",
            "input/domain_glossary.txt",
            "config/domain_glossary.json",
            "domain_glossary.json",
        ]
        
        for path in default_paths:
            if os.path.exists(path):
                return path
        
        return "input/domain_glossary.json"
    
    def load(self) -> Dict[str, str]:
        """ìš©ì–´ì§‘ ë¡œë“œ"""
        if not os.path.exists(self.glossary_path):
            print(f"âš  ìš©ì–´ì§‘ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.glossary_path}")
            print(f"ğŸ“ ê¸°ë³¸ ìš©ì–´ì§‘ì„ ìƒì„±í•©ë‹ˆë‹¤...")
            self._create_default_glossary()
        
        file_extension = Path(self.glossary_path).suffix.lower()
        
        try:
            if file_extension == '.json':
                self.glossary = self._load_json()
            elif file_extension == '.txt':
                self.glossary = self._load_txt()
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_extension}")
            
            print(f"âœ“ ìš©ì–´ì§‘ ë¡œë“œ ì™„ë£Œ: {len(self.glossary)}ê°œ í•­ëª©")
            return self.glossary
            
        except Exception as e:
            print(f"âœ— ìš©ì–´ì§‘ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"ğŸ“ ê¸°ë³¸ ìš©ì–´ì§‘ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return self._get_default_glossary()
    
    def _load_json(self) -> Dict[str, str]:
        """JSON í˜•ì‹ ë¡œë“œ"""
        with open(self.glossary_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
            # ì¤‘ì²© êµ¬ì¡° ì§€ì›
            if "terms" in data:
                return self._flatten_dict(data["terms"])
            return data
    
    def _load_txt(self) -> Dict[str, str]:
        """TXT í˜•ì‹ ë¡œë“œ (í˜•ì‹: í•œêµ­ì–´ = ì˜ë¬¸)"""
        glossary = {}
        with open(self.glossary_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                
                if '=' in line:
                    parts = line.split('=', 1)
                elif ':' in line:
                    parts = line.split(':', 1)
                elif '\t' in line:
                    parts = line.split('\t', 1)
                else:
                    continue
                
                if len(parts) == 2:
                    key = parts[0].strip()
                    value = parts[1].strip()
                    glossary[key] = value
        
        return glossary
    
    def _flatten_dict(self, d: dict) -> Dict[str, str]:
        """ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬ í‰íƒ„í™”"""
        items = {}
        for k, v in d.items():
            if isinstance(v, dict):
                items.update(self._flatten_dict(v))
            else:
                items[k] = v
        return items
    
    def _get_default_glossary(self) -> Dict[str, str]:
        """ê¸°ë³¸ ìš©ì–´ì§‘"""
        return {
            "ìˆ˜ìœ¨": "YIELD",
            "3í˜¸ê¸°": "EQUIPMENT_M03",
            "1í˜¸ê¸°": "EQUIPMENT_M01",
            "2í˜¸ê¸°": "EQUIPMENT_M02",
            "ì§€ë‚œì£¼": "LAST_WEEK",
            "ì´ë²ˆì£¼": "THIS_WEEK",
            "ì™œ ë–¨ì–´ì¡Œì–´": "ANOMALY_ANALYSIS",
            "ì•Œë ¤ì¤˜": "DATA_RETRIEVAL",
            "ë³´ì—¬ì¤˜": "DATA_RETRIEVAL",
            "ìƒì‚°ëŸ‰": "PRODUCTION_VOLUME",
            "ë¶ˆëŸ‰ë¥ ": "DEFECT_RATE"
        }
    
    def _create_default_glossary(self):
        """ê¸°ë³¸ ìš©ì–´ì§‘ íŒŒì¼ ìƒì„±"""
        os.makedirs(os.path.dirname(self.glossary_path) or '.', exist_ok=True)
        
        file_extension = Path(self.glossary_path).suffix.lower()
        
        if file_extension == '.json':
            self._save_json()
        elif file_extension == '.txt':
            self._save_txt()
        
        print(f"âœ“ ê¸°ë³¸ ìš©ì–´ì§‘ ìƒì„±: {self.glossary_path}")
    
    def _save_json(self):
        """JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
        categorized = {
            "metadata": {
                "version": "1.0",
                "description": "ì œì¡° í˜„ì¥ ë„ë©”ì¸ ìš©ì–´ì§‘",
                "last_updated": datetime.now().isoformat()
            },
            "terms": {
                "metrics": {
                    "ìˆ˜ìœ¨": "YIELD",
                    "ìƒì‚°ëŸ‰": "PRODUCTION_VOLUME",
                    "ë¶ˆëŸ‰ë¥ ": "DEFECT_RATE"
                },
                "equipment": {
                    "1í˜¸ê¸°": "EQUIPMENT_M01",
                    "2í˜¸ê¸°": "EQUIPMENT_M02",
                    "3í˜¸ê¸°": "EQUIPMENT_M03"
                },
                "time_expressions": {
                    "ì§€ë‚œì£¼": "LAST_WEEK",
                    "ì´ë²ˆì£¼": "THIS_WEEK"
                },
                "intents": {
                    "ì™œ ë–¨ì–´ì¡Œì–´": "ANOMALY_ANALYSIS",
                    "ì•Œë ¤ì¤˜": "DATA_RETRIEVAL",
                    "ë³´ì—¬ì¤˜": "DATA_RETRIEVAL"
                }
            }
        }
        
        with open(self.glossary_path, 'w', encoding='utf-8') as f:
            json.dump(categorized, f, indent=2, ensure_ascii=False)
    
    def _save_txt(self):
        """TXT í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
        default = self._get_default_glossary()
        
        with open(self.glossary_path, 'w', encoding='utf-8') as f:
            f.write("# ì œì¡° í˜„ì¥ ë„ë©”ì¸ ìš©ì–´ì§‘\n")
            f.write("# í˜•ì‹: í•œêµ­ì–´_ìš©ì–´ = ì˜ë¬¸_ì½”ë“œ\n")
            f.write(f"# ìµœì¢… ì—…ë°ì´íŠ¸: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì •ë¦¬
            f.write("# === ì§€í‘œ (Metrics) ===\n")
            f.write("ìˆ˜ìœ¨ = YIELD\n")
            f.write("ìƒì‚°ëŸ‰ = PRODUCTION_VOLUME\n")
            f.write("ë¶ˆëŸ‰ë¥  = DEFECT_RATE\n\n")
            
            f.write("# === ì„¤ë¹„ (Equipment) ===\n")
            f.write("1í˜¸ê¸° = EQUIPMENT_M01\n")
            f.write("2í˜¸ê¸° = EQUIPMENT_M02\n")
            f.write("3í˜¸ê¸° = EQUIPMENT_M03\n\n")
            
            f.write("# === ì‹œê°„ í‘œí˜„ (Time) ===\n")
            f.write("ì§€ë‚œì£¼ = LAST_WEEK\n")
            f.write("ì´ë²ˆì£¼ = THIS_WEEK\n\n")
            
            f.write("# === ì˜ë„ (Intents) ===\n")
            f.write("ì™œ ë–¨ì–´ì¡Œì–´ = ANOMALY_ANALYSIS\n")
            f.write("ì•Œë ¤ì¤˜ = DATA_RETRIEVAL\n")
            f.write("ë³´ì—¬ì¤˜ = DATA_RETRIEVAL\n")


class QueryNormalizationAgent:
    def __init__(self, 
                 model_id: str = "mistralai/Mistral-7B-Instruct-v0.2", 
                 use_chat: bool = True,
                 glossary_path: Optional[str] = None):
        """
        Args:
            model_id: ì‚¬ìš©í•  LLM ëª¨ë¸ ID
            use_chat: Chat API ì‚¬ìš© ì—¬ë¶€
            glossary_path: ë„ë©”ì¸ ìš©ì–´ì§‘ íŒŒì¼ ê²½ë¡œ
        """
        self.api_token = os.getenv("HUGGINGFACE_API_KEY")
        if not self.api_token:
            raise ValueError("HUGGINGFACE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.model_id = model_id
        self.use_chat = use_chat
        
        # Hugging Face Inference Client ì´ˆê¸°í™”
        try:
            self.client = InferenceClient(token=self.api_token)
            print(f"âœ“ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
            print(f"âœ“ ì‚¬ìš© ëª¨ë¸: {model_id}")
            print(f"âœ“ API ë°©ì‹: {'Chat Completion' if use_chat else 'Text Generation'}")
        except Exception as e:
            print(f"âœ— í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
        
        # ë„ë©”ì¸ ìš©ì–´ì§‘ ë¡œë“œ
        print(f"\n{'='*50}")
        print("ğŸ“š ë„ë©”ì¸ ìš©ì–´ì§‘ ë¡œë”©")
        print(f"{'='*50}")
        
        glossary_loader = DomainGlossaryLoader(glossary_path)
        self.domain_glossary = glossary_loader.load()
        
        # ìš©ì–´ì§‘ ë‚´ìš© ì¶œë ¥
        print("\nğŸ“‹ ë¡œë“œëœ ìš©ì–´ì§‘:")
        for i, (key, value) in enumerate(self.domain_glossary.items(), 1):
            print(f"  {i:2}. {key:15} â†’ {value}")
        print(f"{'='*50}\n")

    def _preprocess_query(self, query: str) -> str:
        """ë„ë©”ì¸ ìš©ì–´ì§‘ ê¸°ë°˜ 1ì°¨ ì •ê·œí™”"""
        processed_query = query
        for k, v in self.domain_glossary.items():
            processed_query = processed_query.replace(k, v)
        return processed_query

    def _construct_system_message(self) -> str:
        """ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìƒì„±"""
        return """You are a Manufacturing Query Normalization Agent.
Convert user queries into JSON objects with these exact fields: intent, metric, time_frame, filter.

Rules:
1. 'intent' must be: ANOMALY_ANALYSIS or DATA_RETRIEVAL
2. 'metric' examples: YIELD, PRODUCTION_VOLUME, DEFECT_RATE
3. 'time_frame' has 'type' (RELATIVE or ABSOLUTE) and 'value'
4. 'filter' has 'field' and 'value'
5. Return ONLY valid JSON without any explanation or markdown

Examples:
Input: "ì§€ë‚œì£¼ 3í˜¸ê¸° ìˆ˜ìœ¨ì´ ì™œ ë–¨ì–´ì¡Œì–´?"
Output: {"intent": "ANOMALY_ANALYSIS", "metric": "YIELD", "time_frame": {"type": "RELATIVE", "value": "LAST_WEEK"}, "filter": {"field": "EQUIPMENT_ID", "value": "EQUIPMENT_M03"}}

Input: "ì´ë²ˆì£¼ 1í˜¸ê¸° ìƒì‚°ëŸ‰ ì•Œë ¤ì¤˜"
Output: {"intent": "DATA_RETRIEVAL", "metric": "PRODUCTION_VOLUME", "time_frame": {"type": "RELATIVE", "value": "THIS_WEEK"}, "filter": {"field": "EQUIPMENT_ID", "value": "EQUIPMENT_M01"}}"""

    def _construct_user_message(self, original_query: str) -> str:
        """ì‚¬ìš©ì ë©”ì‹œì§€ ìƒì„±"""
        return f'Convert this query: "{original_query}"'

    def _call_chat_completion(self, original_query: str) -> str:
        """Chat Completion API í˜¸ì¶œ"""
        messages = [
            {"role": "system", "content": self._construct_system_message()},
            {"role": "user", "content": self._construct_user_message(original_query)}
        ]
        
        response = self.client.chat_completion(
            messages=messages,
            model=self.model_id,
            max_tokens=300,
            temperature=0.1
        )
        
        return response.choices[0].message.content

    def _call_text_generation(self, original_query: str) -> str:
        """Text Generation API í˜¸ì¶œ"""
        prompt = f"""{self._construct_system_message()}

{self._construct_user_message(original_query)}

Output:"""
        
        response = self.client.text_generation(
            prompt,
            model=self.model_id,
            max_new_tokens=300,
            temperature=0.1,
            return_full_text=False
        )
        
        return response

    def _clean_json_output(self, text: str) -> str:
        """LLM ì¶œë ¥ì—ì„œ JSON ì¶”ì¶œ"""
        text = re.sub(r'```json\s*', '', text)
        text = re.sub(r'```\s*', '', text)
        
        json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', text)
        if json_match:
            json_str = json_match.group(0)
            json_str = json_str.replace("'", '"')
            return json_str
        
        start = text.find('{')
        end = text.rfind('}') + 1
        if start != -1 and end > start:
            json_str = text[start:end]
            json_str = json_str.replace("'", '"')
            return json_str
        
        return text

    def normalize(self, query: str, verbose: bool = True) -> Dict[str, Any]:
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
        if verbose:
            print(f"\nğŸ“ ì…ë ¥ ì¿¼ë¦¬: {query}")
        
        pre_processed = self._preprocess_query(query)
        if verbose:
            print(f"ğŸ”„ ì „ì²˜ë¦¬ ì™„ë£Œ: {pre_processed}")
        
        try:
            if verbose:
                print(f"ğŸ¤– LLM í˜¸ì¶œ ì¤‘...")
            
            if self.use_chat:
                response = self._call_chat_completion(query)
            else:
                response = self._call_text_generation(query)
            
            if verbose:
                print(f"âœ“ LLM ì‘ë‹µ ë°›ìŒ")
            
        except Exception as e:
            error_msg = str(e)
            if verbose:
                print(f"âœ— LLM API í˜¸ì¶œ ì‹¤íŒ¨: {error_msg}")
            
            if self.use_chat and "not supported" in error_msg.lower():
                if verbose:
                    print("ğŸ”„ Text Generationìœ¼ë¡œ ì¬ì‹œë„...")
                try:
                    response = self._call_text_generation(query)
                    if verbose:
                        print(f"âœ“ ì¬ì‹œë„ ì„±ê³µ!")
                except Exception as retry_error:
                    return {
                        "error": "All API methods failed",
                        "details": str(retry_error)
                    }
            else:
                return {"error": "LLM API Call Failed", "details": error_msg}

        cleaned_response = self._clean_json_output(response)
        
        try:
            result_json = json.loads(cleaned_response)
            if verbose:
                print("âœ“ JSON íŒŒì‹± ì„±ê³µ")
            return result_json
        except json.JSONDecodeError as e:
            if verbose:
                print(f"âœ— JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            return {
                "error": "Failed to parse JSON",
                "raw_output": response[:500]
            }

    def save_results_to_json(self, results: List[Dict[str, Any]], output_dir: str = "output") -> str:
        """ì •ê·œí™” ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"query_normalization_results_{timestamp}.json"
        filepath = os.path.join(output_dir, filename)
        
        output_data = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "model": self.model_id,
                "api_method": "chat_completion" if self.use_chat else "text_generation",
                "total_queries": len(results),
                "glossary_terms": len(self.domain_glossary)
            },
            "results": results
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}")
        print(f"ğŸ“ íŒŒì¼ í¬ê¸°: {os.path.getsize(filepath)} bytes")
        
        return filepath


# --- ì‹¤í–‰ ---
if __name__ == "__main__":
    print("=" * 50)
    print("ğŸš€ Query Normalization Agent ì‹œì‘")
    print("=" * 50)
    
    model_options = [
        ("mistralai/Mistral-7B-Instruct-v0.2", True),
        ("google/flan-t5-large", False),
    ]
    
    agent = None
    
    for model_id, use_chat in model_options:
        try:
            print(f"\nğŸ” {model_id} ì‹œë„ ì¤‘...")
            agent = QueryNormalizationAgent(
                model_id=model_id, 
                use_chat=use_chat,
                glossary_path="input/domain_glossary.json"  # ë˜ëŠ” .txt
            )
            break
        except Exception as e:
            print(f"âœ— {model_id} ì‹¤íŒ¨: {e}")
            continue
    
    if agent is None:
        print("\nâŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        exit(1)
    
    print(f"\n{'='*50}")
    print("ğŸ’¬ ì‚¬ìš©ì ì…ë ¥ ëª¨ë“œ")
    print("  - ì§ˆë¬¸ ì…ë ¥ í›„ Enter")
    print("  - ì„¸ë¯¸ì½œë¡ (;)ìœ¼ë¡œ ì—¬ëŸ¬ ì§ˆë¬¸ êµ¬ë¶„")
    print("  - ì¢…ë£Œ: q, quit, exit")
    print(f"{'='*50}\n")
    
    all_results = []
    
    while True:
        try:
            user_input = input("ğŸ“Œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: q): ").strip()
            
            if user_input.lower() in ['q', 'quit', 'exit']:
                print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not user_input:
                print("âš  ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\n")
                continue
            
            queries = [q.strip() for q in user_input.split(';') if q.strip()]
            
            for i, query in enumerate(queries, start=len(all_results) + 1):
                print(f"\n{'='*50}")
                print(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i}")
                print(f"{'='*50}")
                
                result = agent.normalize(query, verbose=True)
                
                result_with_query = {
                    "query_id": i,
                    "original_query": query,
                    "normalized_result": result,
                    "timestamp": datetime.now().isoformat()
                }
                
                all_results.append(result_with_query)
                
                print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
                print(json.dumps(result, indent=2, ensure_ascii=False))
            
            print(f"\ní˜„ì¬ê¹Œì§€ {len(all_results)}ê°œì˜ ì§ˆë¬¸ì´ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except KeyboardInterrupt:
            print("\n\nâš  Ctrl+C ê°ì§€. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    
    if all_results:
        saved_path = agent.save_results_to_json(all_results)
        
        print(f"\n{'='*50}")
        print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print(f"ğŸ“‚ ê²°ê³¼ íŒŒì¼: {saved_path}")
        print(f"ğŸ“Š ì´ ì²˜ë¦¬: {len(all_results)}ê°œ")
        print(f"{'='*50}")
    else:
        print("\nâš  ì²˜ë¦¬ëœ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")