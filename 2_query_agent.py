"""
RAG ê¸°ë°˜ SQL ì¿¼ë¦¬ ìƒì„±ê¸°
output í´ë”ì˜ ì •ê·œí™”ëœ JSON ë°ì´í„°ë¥¼ SQL ì¿¼ë¦¬ë¡œ ë³€í™˜
"""

from rag_system import ManufacturingRAGSystem
from typing import Dict, List, Optional
import json
import os
from datetime import datetime


class SQLQueryGenerator:
    """RAG ê¸°ë°˜ SQL ì¿¼ë¦¬ ìƒì„±ê¸°"""
    
    def __init__(self, rag_system: ManufacturingRAGSystem):
        self.rag = rag_system
        
    def parse_normalized_query(self, normalized_data: Dict) -> Dict:
        """ì •ê·œí™”ëœ JSON ë°ì´í„°ë¥¼ SQL ìƒì„±ìš© ì˜ë„ë¡œ ë³€í™˜"""
        intent = {
            "intent_type": normalized_data.get("intent"),
            "metric": normalized_data.get("metric"),
            "time_range": self._parse_time_range(normalized_data.get("time_frame", {})),
            "filter": normalized_data.get("filter"),
            "aggregation": self._infer_aggregation(normalized_data),
            "grouping": self._infer_grouping(normalized_data),
            "ordering": None,
            "limit": None,
            "additional_params": normalized_data.get("additional_params", {})
        }
        
        # ANOMALY_ANALYSISì˜ ê²½ìš° ì •ë ¬ ì¶”ê°€
        if intent["intent_type"] == "ANOMALY_ANALYSIS":
            intent["ordering"] = "DESC"
            intent["limit"] = 10
        
        return intent
    
    def _parse_time_range(self, time_frame: Dict) -> str:
        """ì‹œê°„ í”„ë ˆì„ì„ SQL ì¡°ê±´ìœ¼ë¡œ ë³€í™˜"""
        if not time_frame:
            return None
        
        time_type = time_frame.get("type")
        value = time_frame.get("value")
        
        if time_type == "ABSOLUTE":
            time_mappings = {
                "TODAY": "DATE(CURDATE())",
                "YESTERDAY": "DATE_SUB(CURDATE(), INTERVAL 1 DAY)",
                "THIS_WEEK": "YEARWEEK(CURDATE(), 1)",
                "THIS_MONTH": "DATE_FORMAT(CURDATE(), '%Y-%m')",
                "LAST_MONTH": "DATE_FORMAT(DATE_SUB(CURDATE(), INTERVAL 1 MONTH), '%Y-%m')",
                "LAST_3_MONTHS": "DATE_SUB(CURDATE(), INTERVAL 3 MONTH)",
                "LAST_6_MONTHS": "DATE_SUB(CURDATE(), INTERVAL 6 MONTH)",
                "THIS_YEAR": "YEAR(CURDATE())",
            }
            return time_mappings.get(value, "CURDATE()")
        
        elif time_type == "RELATIVE":
            unit = time_frame.get("unit", "DAY")
            count = time_frame.get("count", 1)
            return f"DATE_SUB(CURDATE(), INTERVAL {count} {unit})"
        
        elif time_type == "RANGE":
            start = time_frame.get("start")
            end = time_frame.get("end")
            return f"BETWEEN '{start}' AND '{end}'"
        
        return None
    
    def _infer_aggregation(self, normalized_data: Dict) -> str:
        """ë©”íŠ¸ë¦­ê³¼ ì˜ë„ì—ì„œ ì§‘ê³„ í•¨ìˆ˜ ì¶”ë¡ """
        intent = normalized_data.get("intent")
        metric = normalized_data.get("metric")
        
        # ì˜ë„ì— ë”°ë¥¸ ì§‘ê³„
        if intent in ["TREND_ANALYSIS", "COMPARISON"]:
            return "AVG"
        elif intent == "AGGREGATION":
            return "SUM"
        elif intent == "ANOMALY_ANALYSIS":
            return "MAX"
        
        # ë©”íŠ¸ë¦­ì— ë”°ë¥¸ ì§‘ê³„
        if metric in ["DEFECT_RATE", "YIELD_RATE", "OEE", "AVAILABILITY"]:
            return "AVG"
        elif metric in ["PRODUCTION_QUANTITY", "INVENTORY", "ENERGY"]:
            return "SUM"
        
        return None
    
    def _infer_grouping(self, normalized_data: Dict) -> str:
        """í•„í„°ì™€ ì˜ë„ì—ì„œ ê·¸ë£¹í•‘ ì¶”ë¡ """
        intent = normalized_data.get("intent")
        filter_data = normalized_data.get("filter", {})
        
        # í•„í„°ê°€ ìˆìœ¼ë©´ í•´ë‹¹ í•„ë“œë¡œ ê·¸ë£¹í•‘
        if filter_data and filter_data.get("field"):
            field = filter_data.get("field")
            field_mappings = {
                "LINE_ID": "line_id",
                "PRODUCT_ID": "product_id",
                "EQUIPMENT_ID": "equipment_id",
                "FACTORY_ID": "factory_id",
                "DEPARTMENT_ID": "department_id",
                "SHIFT": "shift",
                "SUPPLIER_ID": "supplier_id",
            }
            return field_mappings.get(field, field.lower())
        
        # ì¶”ì„¸ ë¶„ì„ì˜ ê²½ìš° ì‹œê°„ë³„ ê·¸ë£¹í•‘
        if intent == "TREND_ANALYSIS":
            time_frame = normalized_data.get("time_frame", {})
            if "MONTH" in str(time_frame.get("value", "")):
                return "DATE_FORMAT(production_date, '%Y-%m')"
            elif "WEEK" in str(time_frame.get("value", "")):
                return "YEARWEEK(production_date, 1)"
            else:
                return "DATE(production_date)"
        
        return None
    
    def generate_sql_from_json(self, json_data: Dict) -> Dict:
        """ì •ê·œí™”ëœ JSON ë°ì´í„°ë¡œë¶€í„° SQL ìƒì„±"""
        
        # 1. JSONì—ì„œ ì •ê·œí™”ëœ ì¿¼ë¦¬ ì¶”ì¶œ
        if "results" in json_data and len(json_data["results"]) > 0:
            query_result = json_data["results"][0]
            normalized_data = query_result.get("normalized_result", {})
            original_query = query_result.get("original_query", "")
        else:
            normalized_data = json_data
            original_query = ""
        
        # 2. ì˜ë„ íŒŒì‹±
        intent = self.parse_normalized_query(normalized_data)
        
        # 3. RAG ê²€ìƒ‰ (ì›ë³¸ ì¿¼ë¦¬ ë˜ëŠ” ë©”íŠ¸ë¦­ìœ¼ë¡œ)
        search_query = original_query if original_query else intent.get("metric", "")
        results = self.rag.search_all(search_query, top_k=3)
        
        # 4. Golden SQLì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ì¿¼ë¦¬ ì°¾ê¸°
        best_golden_sql = None
        best_score = 0
        for doc, score in results['golden_sql']:
            if score > best_score:
                best_score = score
                best_golden_sql = doc.metadata
        
        # 5. ê´€ë ¨ ìŠ¤í‚¤ë§ˆ ì •ë³´ ìˆ˜ì§‘
        relevant_tables = []
        for doc, score in results['schema']:
            if score > 0.1:
                relevant_tables.append(doc.metadata)
        
        # 6. ê´€ë ¨ KPI ì •ë³´ ìˆ˜ì§‘
        relevant_kpis = []
        for doc, score in results['kpi']:
            if score > 0.1:
                relevant_kpis.append(doc.metadata)
        
        # 7. ë©”íŠ¸ë¦­ ê¸°ë°˜ KPI ë§¤ì¹­
        metric = intent.get("metric")
        if metric and not relevant_kpis:
            metric_to_kpi = {
                "DEFECT_RATE": "ë¶ˆëŸ‰ë¥ ",
                "YIELD_RATE": "ìˆ˜ìœ¨",
                "OEE": "OEE",
                "AVAILABILITY": "ê°€ë™ë¥ ",
                "PRODUCTION_QUANTITY": "ìƒì‚°ëŸ‰",
                "THROUGHPUT": "ì²˜ë¦¬ëŸ‰",
                "MTBF": "MTBF",
                "MTTR": "MTTR",
            }
            kpi_name = metric_to_kpi.get(metric)
            if kpi_name:
                kpi_results = self.rag.search_all(kpi_name, top_k=1)
                for doc, score in kpi_results['kpi']:
                    if score > 0.1:
                        relevant_kpis.append(doc.metadata)
        
        # 8. ê²°ê³¼ êµ¬ì„±
        result = {
            "original_query": original_query,
            "normalized_data": normalized_data,
            "intent_analysis": intent,
            "relevant_tables": [t['table_name'] for t in relevant_tables],
            "relevant_kpis": [k['kpi_name_kr'] for k in relevant_kpis],
            "recommended_sql": None,
            "sql_source": None,
            "confidence": 0,
            "explanation": ""
        }
        
        # 9. SQL ìƒì„± ë˜ëŠ” ì¶”ì²œ
        if best_golden_sql and best_score > 0.3:
            # Golden SQL í™œìš©
            result["recommended_sql"] = self._adapt_golden_sql(best_golden_sql['sql'], intent)
            result["sql_source"] = "golden_sql"
            result["confidence"] = min(best_score * 100, 95)
            result["explanation"] = f"Golden SQL '{best_golden_sql['query_id']}'ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±. {best_golden_sql['explanation']}"
        elif relevant_kpis and relevant_kpis[0].get('sql_example'):
            # KPI SQL ì˜ˆì‹œ í™œìš©
            result["recommended_sql"] = self._adapt_kpi_sql(relevant_kpis[0]['sql_example'], intent)
            result["sql_source"] = "kpi_example"
            result["confidence"] = 70
            result["explanation"] = f"KPI '{relevant_kpis[0]['kpi_name_kr']}' ê³„ì‚° ì˜ˆì‹œ SQL í™œìš©"
        else:
            # ê¸°ë³¸ ì¿¼ë¦¬ ìƒì„±
            result["recommended_sql"] = self._generate_basic_sql(intent, relevant_tables, normalized_data)
            result["sql_source"] = "generated"
            result["confidence"] = 50
            result["explanation"] = "ì •ê·œí™”ëœ ë°ì´í„°ì™€ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸°ë³¸ ì¿¼ë¦¬ ìƒì„±"
        
        return result
    
    def _adapt_golden_sql(self, base_sql: str, intent: Dict) -> str:
        """Golden SQLì„ ì˜ë„ì— ë§ê²Œ ì¡°ì •"""
        sql = base_sql
        
        # ì‹œê°„ ë²”ìœ„ ì¡°ì •
        if intent.get("time_range"):
            # WHERE ì ˆ ìˆ˜ì • ë˜ëŠ” ì¶”ê°€
            if "WHERE" in sql.upper():
                sql = sql.replace("WHERE", f"WHERE production_date >= {intent['time_range']} AND")
            else:
                sql += f"\nWHERE production_date >= {intent['time_range']}"
        
        # í•„í„° ì¡°ì •
        filter_data = intent.get("filter")
        if filter_data:
            field = filter_data.get("field", "").lower()
            value = filter_data.get("value")
            if "WHERE" in sql.upper():
                sql += f"\n  AND {field} = '{value}'"
            else:
                sql += f"\nWHERE {field} = '{value}'"
        
        return sql
    
    def _adapt_kpi_sql(self, base_sql: str, intent: Dict) -> str:
        """KPI SQLì„ ì˜ë„ì— ë§ê²Œ ì¡°ì •"""
        return self._adapt_golden_sql(base_sql, intent)
    
    def _generate_basic_sql(self, intent: Dict, tables: List[Dict], normalized_data: Dict) -> str:
        """ê¸°ë³¸ SQL ì¿¼ë¦¬ ìƒì„±"""
        
        # ë©”íŠ¸ë¦­ì— ë”°ë¥¸ í…Œì´ë¸” ì„ íƒ
        metric = intent.get("metric")
        table_mappings = {
            "DEFECT_RATE": "quality_inspection",
            "YIELD_RATE": "production_results",
            "OEE": "equipment_performance",
            "PRODUCTION_QUANTITY": "production_results",
            "INVENTORY": "inventory",
            "MTBF": "equipment_failures",
            "MTTR": "equipment_failures",
        }
        
        table_name = table_mappings.get(metric)
        if not table_name and tables:
            table_name = tables[0]['table_name']
        elif not table_name:
            table_name = "production_results"
        
        # SELECT ì ˆ êµ¬ì„±
        select_cols = []
        grouping = intent.get("grouping")
        
        if grouping:
            select_cols.append(grouping)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        aggregation = intent.get("aggregation", "AVG")
        if metric == "DEFECT_RATE":
            select_cols.append(f"{aggregation}(defect_quantity / total_quantity * 100) AS defect_rate")
        elif metric == "OEE":
            select_cols.append(f"{aggregation}(oee) AS avg_oee")
        elif metric == "PRODUCTION_QUANTITY":
            select_cols.append(f"{aggregation}(production_quantity) AS total_production")
        else:
            select_cols.append(f"{aggregation}(*) AS value")
        
        select_clause = ", ".join(select_cols) if select_cols else "*"
        
        # ê¸°ë³¸ ì¿¼ë¦¬ êµ¬ì„±
        sql = f"SELECT {select_clause}\nFROM {table_name}"
        
        # WHERE ì ˆ
        where_conditions = []
        
        # ì‹œê°„ ë²”ìœ„
        if intent.get("time_range"):
            where_conditions.append(f"production_date >= {intent['time_range']}")
        
        # í•„í„°
        filter_data = intent.get("filter")
        if filter_data:
            field = filter_data.get("field", "").lower()
            value = filter_data.get("value")
            where_conditions.append(f"{field} = '{value}'")
        
        if where_conditions:
            sql += "\nWHERE " + " AND ".join(where_conditions)
        
        # GROUP BY ì ˆ
        if grouping and aggregation:
            sql += f"\nGROUP BY {grouping}"
        
        # ORDER BY ì ˆ
        if intent.get("ordering"):
            order_col = "value" if not metric else select_cols[-1].split(" AS ")[-1]
            sql += f"\nORDER BY {order_col} {intent['ordering']}"
        
        # LIMIT ì ˆ
        if intent.get("limit"):
            sql += f"\nLIMIT {intent['limit']}"
        
        return sql


def process_json_file(json_file_path: str, rag_system: ManufacturingRAGSystem) -> Dict:
    """JSON íŒŒì¼ì„ ì½ì–´ì„œ SQL ìƒì„±"""
    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        
        generator = SQLQueryGenerator(rag_system)
        result = generator.generate_sql_from_json(json_data)
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        if "metadata" in json_data:
            result["metadata"] = json_data["metadata"]
        
        return result
    
    except Exception as e:
        return {
            "error": str(e),
            "json_file": json_file_path
        }


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 80)
    print("ğŸ­ ì œì¡°ì—… RAG ê¸°ë°˜ SQL ì¿¼ë¦¬ ìƒì„±ê¸° (JSON ì…ë ¥ ë²„ì „)")
    print("=" * 80)
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    print("\n[1] RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    rag_system = ManufacturingRAGSystem()
    
    # ë°ì´í„° ë¡œë“œ (ê²½ë¡œ í™•ì¸ í•„ìš”)
    try:
        rag_system.load_schema_data("./input/RAG/schema_Info.json")
        rag_system.load_kpi_data("./input/RAG/kpi.json")
        rag_system.load_golden_sql_data("./input/RAG/goldenSQL.json")
        print("   âœ… RAG ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"   âš ï¸ RAG ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("   â„¹ï¸ ê¸°ë³¸ ì¿¼ë¦¬ ìƒì„± ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
    
    # output í´ë”ì—ì„œ JSON íŒŒì¼ ì°¾ê¸°
    output_dir = "output"
    if not os.path.exists(output_dir):
        print(f"\nâŒ ì˜¤ë¥˜: '{output_dir}' í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    json_files = [f for f in os.listdir(output_dir) if f.endswith('.json')]
    
    if not json_files:
        print(f"\nâŒ ì˜¤ë¥˜: '{output_dir}' í´ë”ì— JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì„ íƒ
    json_files.sort(reverse=True)
    latest_file = os.path.join(output_dir, json_files[0])
    
    print(f"\n[2] JSON íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
    print(f"   ğŸ“ íŒŒì¼: {latest_file}")
    
    # SQL ìƒì„±
    result = process_json_file(latest_file, rag_system)
    
    if "error" in result:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {result['error']}")
        return
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“ ì›ë³¸ ì§ˆì˜")
    print("=" * 80)
    print(result.get("original_query", "N/A"))
    
    print("\n" + "=" * 80)
    print("ğŸ“Š ì •ê·œí™”ëœ ë°ì´í„°")
    print("=" * 80)
    print(json.dumps(result["normalized_data"], indent=2, ensure_ascii=False))
    
    print("\n" + "=" * 80)
    print("ğŸ” ì˜ë„ ë¶„ì„")
    print("=" * 80)
    intent = result["intent_analysis"]
    print(f"   ì˜ë„ ìœ í˜•: {intent.get('intent_type')}")
    print(f"   ë©”íŠ¸ë¦­: {intent.get('metric')}")
    print(f"   ì‹œê°„ë²”ìœ„: {intent.get('time_range')}")
    print(f"   í•„í„°: {intent.get('filter')}")
    print(f"   ì§‘ê³„í•¨ìˆ˜: {intent.get('aggregation')}")
    print(f"   ê·¸ë£¹í•‘: {intent.get('grouping')}")
    print(f"   ì •ë ¬: {intent.get('ordering')}")
    print(f"   LIMIT: {intent.get('limit')}")
    
    print("\n" + "=" * 80)
    print("ğŸ” RAG ê²€ìƒ‰ ê²°ê³¼")
    print("=" * 80)
    print(f"   ê´€ë ¨ í…Œì´ë¸”: {', '.join(result['relevant_tables'][:5]) if result['relevant_tables'] else 'N/A'}")
    print(f"   ê´€ë ¨ KPI: {', '.join(result['relevant_kpis'][:5]) if result['relevant_kpis'] else 'N/A'}")
    
    print("\n" + "=" * 80)
    print(f"ğŸ’¡ ì¶”ì²œ SQL (ì¶œì²˜: {result['sql_source']}, ì‹ ë¢°ë„: {result['confidence']:.0f}%)")
    print("=" * 80)
    if result['recommended_sql']:
        sql_lines = result['recommended_sql'].split('\n')
        for line in sql_lines:
            print(f"   {line}")
    else:
        print("   SQL ìƒì„± ì‹¤íŒ¨")
    
    print("\n" + "=" * 80)
    print("ğŸ“Œ ì„¤ëª…")
    print("=" * 80)
    print(f"   {result['explanation']}")
    
    # ë©”íƒ€ë°ì´í„° ì¶œë ¥
    if result.get("metadata"):
        print("\n" + "=" * 80)
        print("â„¹ï¸ ë©”íƒ€ë°ì´í„°")
        print("=" * 80)
        metadata = result["metadata"]
        print(f"   íƒ€ì„ìŠ¤íƒ¬í”„: {metadata.get('timestamp', 'N/A')}")
        print(f"   ëª¨ë¸: {metadata.get('model', 'N/A')}")
        print(f"   ì´ ì¿¼ë¦¬ ìˆ˜: {metadata.get('total_queries', 'N/A')}")
        print(f"   ìš©ì–´ì§‘ í¬ê¸°: {metadata.get('glossary_terms', 'N/A')}")
    
    print("\n" + "=" * 80)


if __name__ == "__main__":
    main()